library(fs)
library(here)
## Run the scripts -------------------------------------------------------------
dir_info()
?dir_info
dir_ls()
dir_ls("[0-9?-]")
dir_ls() %>% grep("0[0-9]-")
with(dir_ls(), grep("0[0-9]-"))
grep("0[0-9]-", dir_ls())
source('~/Dropbox/AnalysisScripts/ReproduceAnalyses-Tucker_et_al_2020.R')
rm(list=ls())
## Run the scripts -------------------------------------------------------------
source("01-Experiment1-Analysis-Frontiers.R")
source("02-Experiment1-Analysis-Frontiers.R")
source("02-Experiment2-Analysis-Frontiers.R")
?across
source('~/Dropbox/AnalysisScripts/02-Experiment2-Analysis-Frontiers.R')
source('~/Dropbox/AnalysisScripts/ReproduceAnalyses-Tucker_et_al_2020.R')
source('~/Dropbox/AnalysisScripts/03-Experiment3-Analysis-Frontiers.R')
?file_delete
source('~/Dropbox/AnalysisScripts/06-PrepareTucker2015.R')
source('~/Dropbox/AnalysisScripts/06-PrepareTucker2015.R')
source('~/Dropbox/AnalysisScripts/06-PrepareTucker2015.R')
source('~/Dropbox/AnalysisScripts/ReproduceAnalyses-Tucker_et_al_2020.R')
source('~/Dropbox/JML-Revision/AnalysisScripts/00-ReproduceAnalyses-Tucker_et_al_2020.R')
remotes::install_github("karthik/holepunch")
install.packages("remotes")
remotes::install_github("karthik/holepunch")
library(holepunch)
write_compendium_description(package = "TuckerIdrissiAlmeida2020",
description = "Reproducibility package for the
research reported in Tucker, Idrissi, & Almeida
(2020).Attraction effects for verbal gender and
number are similar but not identical: self-paced
reading evidence from Modern Standard Arabic.
Frontiers in Psychology, Language Sciences.
doi: 10.3389/fpsyg.2020.586464")
write_dockerfile(maintainer = "diogo-almeida")
# To write a Dockerfile. It will automatically pick the date of the last
# modified file, match it to that version of R and add it here. You can
# override this by passing r_date to some arbitrary date
# (but one for which a R version exists).
generate_badge() # This generates a badge for your readme.
# And click on the badge or use the function below to get the build
# ready ahead of time.
build_binder()
write_dockerfile()
write_dockerfile(maintainer = "diogo-almeida")
?write_dockerfile
write_dockerfile(maintainer = "diogo-almeida", branch = "main")
generate_badge() # This generates a badge for your readme.
# And click on the badge or use the function below to get the build
# ready ahead of time.
build_binder()
# And click on the badge or use the function below to get the build
# ready ahead of time.
build_binder()
generate_badge() # This generates a badge for your readme.
generate_badge() # This generates a badge for your readme.
library(holepunch)
write_dockerfile(maintainer = "diogo-almeida", branch = "main")
generate_badge() # This generates a badge for your readme.
?write_dockerfile
write_dockerfile(maintainer = "diogo-almeida", branch = "main",
r_date = "2020-04-24")
generate_badge() # This generates a badge for your readme.
# And click on the badge or use the function below to get the build
# ready ahead of time.
build_binder()
write_dockerfile(maintainer = "diogo-almeida", branch = "main",
r_date = "2020-02-29")
write_dockerfile(maintainer = "diogo-almeida", branch = "main",
r_date = "2020-04-03")
write_dockerfile(maintainer = "diogo-almeida", branch = "main",
r_date = "2020-04-23")
write_dockerfile(maintainer = "diogo-almeida", branch = "main",
r_date = "2020-04-23", install_github = TRUE)
?write_dockerfile
write_dockerfile(maintainer = "diogo-almeida", branch = "main",
r_date = "2020-04-23", install_github = TRUE)
write_dockerfile(maintainer = "diogo-almeida", branch = "main",
r_date = "2020-04-23", install_github = TRUE)
??ggsav
??ggsave
?ggsave
library(tidyverse)
library(viridis)
library(ragg)
library(pins)
library(fs)
library(here)
library(stargazer)
library(bootES)
lamb.winsorize <- function(x, cut.off = 0.1, cut.off.unit = "percent") {
if (is.list(x)) {
x <- unlist(x)
}
winsorized.x <- x
if (cut.off.unit == "sd") {
sdx <- sd(x)
mx  <- mean(x)
lowerx <- mx - (cut.off * sdx)
upperx <- mx + (cut.off * sdx)
} else {
if (cut.off.unit == "percent") {
len.x <- length(x)
x.ascending <- sort(x)
g <- trunc(cut.off * len.x)
lowerx <- x.ascending[g + 1]
upperx <- x.ascending[len.x - g]
} else {
stop("'cut.off.unit' must be 'sd' (standard deviation) or 'percent'!")
}
}
winsorized.x[x <= lowerx] <- lowerx
winsorized.x[x >= upperx] <- upperx
return(winsorized.x)
}
lamb.sem <- function(x) {
if (any(is.na(x))){
x <- x[!is.na(x)]
}
return(sqrt(var(x)/length(x)))
}
## plotting variables ----------------------------------------------------------
manuscript.spr.plot.theme <- theme_bw() +
theme(legend.key = element_blank(),
plot.background = element_blank(),
plot.title = element_text(hjust = 0.5),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
axis.line = element_line(color = 'black'),
legend.position="bottom",
legend.title = element_blank())
spr.rt.width  <-  6.7
spr.rt.height <-  3.7
## Set variable paths for data, figures and tables produced by analysis --------
this_exp <- "exp1"
data.dir <- here(this_exp, "data")
figs.dir <- here(this_exp, "figures")
tbls.dir <- here(this_exp, "tables")
meta.dir <- here("meta")
dir_create(c(data.dir, figs.dir, tbls.dir, meta.dir))
## Getting data ----------------------------------------------------------------
exp1.datafile.figshare <- "https://ndownloader.figshare.com/files/25914762"
pin(exp1.datafile.figshare, name = "experiment1_data")
exp1.original <- pin_get("experiment1_data") %>%
readr::read_csv()
## Saving a copy of the original dataset on the data folder --------------------
exp1.original %>%
readr::write_csv(here(data.dir, "experiment1.csv"))
## Set parameters for data analysis --------------------------------------------
wins.cutoff  <- .01   # 1% cutoff
error.cutoff <- .5    # Original Analysis = .5; Alternative = .67 (roughly 2/3)
## Regions of Interest ---------------------------------------------------------
rois <- c("Verb", "Verb+1", "Verb+2")
exp1.regions <- c("NP Subj", "Comp", "RC Verb", "Attr", "Adverb", "Verb",
"Verb+1", "Verb+2")
## Exclude Fillers, Incorrect trials, NAs, and Regions after Verb+2 ------------
exp1.data.ok <- exp1.original %>%
filter(Correct == 1 & Condition != "Filler" & Region %in% exp1.regions &
!is.na(RT))
## Winsorizing the data at 1% --------------------------------------------------
exp1.data.ok <- exp1.data.ok %>%
group_by(Condition, Region) %>%
mutate(RTw01 = lamb.winsorize(RT, cut.off = wins.cutoff,
cut.off.unit = "percent")) %>%
ungroup()
## Subject averages ------------------------------------------------------------
exp1.data.saves <- exp1.data.ok %>%
group_by(SubjID, Condition, Region) %>%
summarise(meanRT = mean(RTw01, na.rm = TRUE)) %>%
ungroup()
## Find subjects who had empty cells -------------------------------------------
## i.e., who had conditions in which no average data can be computed given our
## exclusion criteria
exp1.subjects.empty.cells <- exp1.data.saves %>%
group_by(Region, SubjID) %>%
summarise(count = n()) %>%
ungroup() %>%
filter(count < 4) %>%
select(SubjID) %>%
distinct() %>%
unlist() %>%
as.vector()
exp1.bad.subjects <- exp1.original %>%
filter(Condition != "Filler" & Region %in% exp1.regions) %>%
group_by(SubjID) %>%
summarise(MeanCorrect = mean(Correct, na.rm = TRUE)) %>%
ungroup() %>%
filter(MeanCorrect < error.cutoff) %>%
select(SubjID)
## Excluding subjects with empty cells and who answered less than 50% correct --
exp1.data.ok <- exp1.data.ok %>%
filter(!SubjID %in% exp1.bad.subjects) %>%
droplevels()
exp1.data.saves <- exp1.data.saves %>%
filter(!SubjID %in% exp1.subjects.empty.cells &
!SubjID %in% exp1.bad.subjects) %>%
droplevels()
## Grand Averages --------------------------------------------------------------
exp1.data.gdave <- exp1.data.saves %>%
group_by(Condition, Region) %>%
summarise(gd.avg.RT = mean(meanRT, na.rm = TRUE),
gd.avg.RT.SE = lamb.sem(meanRT),
se.min = gd.avg.RT - gd.avg.RT.SE,
se.max = gd.avg.RT + gd.avg.RT.SE) %>%
separate(Condition, into = c("Match", "Grammaticality"), sep = "/",
remove = FALSE) %>%
ungroup() %>%
mutate(SubjPhi = rep("Gend", time = length(Match))) %>%
mutate(across(where(is.character), factor)) %>%
mutate(Experiment = rep("exp1.A", times = length(SubjPhi))) %>%
droplevels()
## Means & Variances Tables for LaTeX ------------------------------------------
regions.for.table <- c("Verb", "Verb+1", "Verb+2")
exp1.means.latex <- exp1.data.gdave %>%
filter(Region %in% regions.for.table) %>%
select(Condition, Region, gd.avg.RT, gd.avg.RT.SE) %>%
transmute(Cond = as.character(Condition),
Region = Region,
Average = round(gd.avg.RT, 0),
SE = round(gd.avg.RT.SE, 0)) %>%
arrange(Region)
colnames(exp1.means.latex) <- c("Condition", "Region", "Mean", "SE")
exp1.means.latex %>% select(Condition, Mean, SE) %>%
stargazer::stargazer(summary=FALSE, rownames=FALSE, keep=c(1, 2, 3),
out=here(tbls.dir, "exp1-means.tex"))
## Grand Average Plotting ------------------------------------------------------
exp1.grand.average.plot <- ggplot(exp1.data.gdave,
aes(x = Region, y = gd.avg.RT,
group = Condition,
colour = Condition))
plot.manuscript.all <- exp1.grand.average.plot +
labs(x = "Region", y = "Raw RT (ms)",
title = "Experiment 1: Gender Attraction, Masculine Subjects") +
geom_line(aes(linetype = Condition)) +
geom_point(aes(shape = Condition), size = 3) +
geom_errorbar(aes(ymax = se.max, ymin = se.min), width=0.1) +
scale_shape_manual(values = c("Match/Gram" = 15,
"Match/Ungram" = 15,
"NoMatch/Gram" = 0,
"NoMatch/Ungram" = 0)) +
scale_linetype_manual(values = c("NoMatch/Gram" = 1,
"Match/Gram" = 1,
"NoMatch/Ungram" = 2,
"Match/Ungram" = 2)) +
scale_colour_manual(values = c("NoMatch/Gram" = viridis_pal()(10)[7],
"Match/Gram" = viridis_pal()(10)[7],
"NoMatch/Ungram" = viridis_pal()(10)[2],
"Match/Ungram" = viridis_pal()(10)[2])) +
manuscript.spr.plot.theme + guides(shape = guide_legend(nrow = 1,
byrow = TRUE)) +
annotate("rect", xmin = 5.5, xmax = 8.5, ymin = -Inf, ymax = Inf, alpha = 0.2,
fill = "grey")
?agg_jpg
?agg_jpeg
?eps
?utils::eps
?postscript
rm(list=ls())
source('~/Dropbox/JML-Revision/AnalysisScripts/00-ReproduceAnalyses-Tucker_et_al_2020.R')
library(tidyverse)
library(viridis)
library(ragg)
library(pins)
library(fs)
library(here)
library(stargazer)
library(metafor)
## Loading data for meta analysis ----------------------------------------------
this_exp <- "meta"
figs.dir <- here(this_exp, "figures")
dir_create(figs.dir)
meta.analysis.rdata.files <- c("Experiment1-DataForMetaAnalysis.RData",
"Experiment2-DataForMetaAnalysis.RData",
"Experiment3-DataForMetaAnalysis.RData",
"Experiment4-DataForMetaAnalysis.RData",
"Experiment5-DataForMetaAnalysis.RData",
"Tucker2015-DataForMetaAnalysis.RData")
## Check if files exist locally, otherwise download them from figshare ---------
if (all(file_exists(here(this_exp, meta.analysis.rdata.files)))) {
lapply(here(this_exp, meta.analysis.rdata.files), load, envir = .GlobalEnv)
} else {
meta.datafile.figshare <- "https://ndownloader.figshare.com/files/25894410"
pin(meta.datafile.figshare, name = "MetaAnalysisDatasets-small.zip")
pin_get("MetaAnalysisDatasets-small.zip") %>%
unzip() %>%
load(envir = .GlobalEnv, verbose = TRUE)
}
## Regions of Interest for Analysis --------------------------------------------
meta.regions <- c("Verb", "Verb+1", "Verb+2")
## Variables for meta analysis - plotting --------------------------------------
chrono.order <- c("Tucker2015.A", "Tucker2015.B", "Exp1.A", "Exp2.A", "Exp3.A",
"Exp3.B", "Exp4.A", "Exp5.A", "Exp2.B", "Exp4.B", "Exp5.B")
exp.labels <- c("Exp1.A", "Exp2.A", "Exp2.B", "Exp3.A", "Exp3.B", "Exp4.A",
"Exp4.B", "Exp5.A", "Exp5.B", "Tucker2015.A", "Tucker2015.B")
exp.plotlabels <- c("1", "2A", "2B", "3: BrkPlAmb",
"3: BrkPlUnamb", "4A", "4B", "5A", "5B",
"T15: SndPl", "T15: BrkPl")
## Prepare Experiment 1 --------------------------------------------------------
exp1 <- exp1.data.saves %>%
mutate(Experiment = factor(rep("Exp1", times = length(SubjID))),
SubExperiment = factor(rep("A", times = length(SubjID))),
SubjPhi = factor(rep("Masc", times = length(SubjID)))) %>%
separate(Condition, into = c("Match", "Grammaticality"))
## Prepare Experiment 2 --------------------------------------------------------
exp2 <- exp2.data.saves %>%
mutate(Experiment = factor(rep("Exp2", times = length(SubjID)))) %>%
separate(Condition, into = c("SubjPhi", "Match", "Grammaticality"))
## Prepare Experiment 3 --------------------------------------------------------
exp3 <- exp3.data.saves %>%
mutate(Experiment = factor(rep("Exp3", times = length(SubjID))),
SubExperiment = factor(if_else(Ambiguity == "Ambiguous", "A", "B")),
SubjPhi = factor(rep("Sg", times = length(SubjID)))) %>%
separate(Condition, into = c("Match", "Grammaticality"))
## Prepare Experiment 4 --------------------------------------------------------
exp4 <- exp4.data.saves %>%
mutate(Experiment = factor(rep("Exp4", times = length(SubjID)))) %>%
separate(Condition, into = c("SubjPhi", "Match", "Grammaticality"))
## Prepare Experiment 5 --------------------------------------------------------
exp5 <- exp5.data.saves %>%
mutate(Experiment = factor(rep("Exp5", times = length(SubjID)))) %>%
separate(Condition, into = c("PhiFeature", "Match", "Grammaticality")) %>%
mutate(SubjPhi = factor(if_else(PhiFeature == "Num", "Sg", "Masc")))
## Prepare Tucker 2015 ---------------------------------------------------------
tucker2015 <- tucker2015.data.saves %>%
mutate(Experiment = factor(rep("Tucker2015", times = length(SubjID))),
SubExperiment = factor(if_else(Gender == "Fem", "A", "B")),
SubjPhi = factor(rep("Sg", times = length(SubjID)))) %>%
separate(Condition, into = c("Match", "Grammaticality"))
## combine datasets ------------------------------------------------------------
meta.data <- bind_rows(exp1, exp2, exp3, exp4, exp5, tucker2015) %>%
mutate(SubjPhiType = factor(if_else(SubjPhi %in%
c("Sg", "Pl"), "Num", "Gend")))%>%
unite(., "Condition", Match, Grammaticality) %>%
unite(., "UniqueSubjID", SubjID, Experiment, SubExperiment, remove = FALSE) %>%
spread(Condition, meanRT)
## Calculate the Grammaticality Effect in each experiment ----------------------
set.seed(1234)
meta.gram <- meta.data %>%
mutate(GrammaticalityEffect = (Match_Ungram + NoMatch_Ungram) -
(Match_Gram + NoMatch_Gram)) %>%
group_by(Experiment, SubExperiment, SubjPhi, Region) %>%
summarise(MeanEffect = mean(GrammaticalityEffect),
SDEffect = sd(GrammaticalityEffect),
N = n(),
VarEffect = (SDEffect^2)/N,
StudyWeightEffect = 1 / VarEffect,
CIEffect = paste(bootES(GrammaticalityEffect)$bounds,
collapse = "/")) %>%
mutate(Exp = factor(paste(Experiment, SubExperiment, sep = "."))) %>%
separate(CIEffect, into = c("CI.min", "CI.max"), sep = "/", convert = TRUE) %>%
mutate(EffectType = factor(rep("Grammaticality", times = length(Exp))))
## Calculate the Attraction Effect in each experiment within Ungramm sentences -
meta.attr.ungram <- meta.data %>%
mutate(IntrusionU = Match_Ungram - NoMatch_Ungram) %>%
group_by(Experiment, SubExperiment, SubjPhi, Region) %>%
summarise(MeanEffect = mean(IntrusionU),
SDEffect = sd(IntrusionU),
N = n(),
VarEffect = (SDEffect^2)/N,
StudyWeightEffect = 1 / VarEffect,
CIEffect = paste(bootES(IntrusionU)$bounds, collapse = "/")) %>%
mutate(Exp = factor(paste(Experiment, SubExperiment, sep = "."))) %>%
separate(CIEffect, into = c("CI.min", "CI.max"), sep = "/", convert = TRUE) %>%
mutate(EffectType = factor(rep("Attraction.U", times = length(Exp))))
## Calculate the Attraction Effect in each experiment within Gramm sentences ---
meta.attr.gram <- meta.data %>%
mutate(IntrusionG = Match_Gram - NoMatch_Gram) %>%
group_by(Experiment, SubExperiment, SubjPhi, Region) %>%
summarise(MeanEffect = mean(IntrusionG),
SDEffect = sd(IntrusionG),
N = n(),
VarEffect = (SDEffect^2)/N,
StudyWeightEffect = 1 / VarEffect,
CIEffect = paste(bootES(IntrusionG)$bounds, collapse = "/")) %>%
mutate(Exp = factor(paste(Experiment, SubExperiment, sep = "."))) %>%
separate(CIEffect, into = c("CI.min", "CI.max"), sep = "/", convert = TRUE) %>%
mutate(EffectType = factor(rep("Attraction.G", times = length(Exp))))
## Combining Grammatical and Attraction Effects into single dataset ------------
meta.studies <- bind_rows(meta.gram, meta.attr.gram, meta.attr.ungram)
## Calculate the Meta Analyis Estimates ----------------------------------------
## This uses the correct formulas
## same output as in package metafor
meta.estimates <- meta.studies %>%
group_by(EffectType, SubjPhi, Region) %>%
summarise(MeanEffect = sum(MeanEffect * StudyWeightEffect) /
sum(StudyWeightEffect),
CI.min = MeanEffect - (1.96 * sqrt(1/sum(StudyWeightEffect))),
CI.max = MeanEffect + (1.96 * sqrt(1/sum(StudyWeightEffect)))) %>%
mutate(Exp = factor(rep("MetaAnalysis", times = length(SubjPhi))))
## Attraction Effect
meta.estimates %>%
filter(Region %in% meta.regions) %>%
filter(EffectType == "Attraction.U")
## Attraction Effect on Subject
meta.estimates %>%
filter(Region %in% meta.regions) %>%
filter(EffectType == "Attraction.G")
meta.estimates %>%
filter(Region %in% meta.regions) %>%
filter(EffectType == "Grammaticality")
## Meta Analysis Calculation and Plotting using package metafor ----------------
## Easy forest plot functions
### Number Attraction ----------------------------------------------------------
### Ungrammatical Sentences
### Singular and Plural Subjects
### Verb, Verb+1 and Verb+2 regions
meta.attr.sg.V.u <- meta.studies %>%
filter(Region == "Verb", EffectType == "Attraction.U", SubjPhi %in% c("Sg"))
meta.attr.sg.V1.u <- meta.studies %>%
filter(Region == "Verb+1", EffectType == "Attraction.U", SubjPhi %in% c("Sg"))
meta.attr.sg.V2.u <- meta.studies %>%
filter(Region == "Verb+2", EffectType == "Attraction.U", SubjPhi %in% c("Sg"))
meta.attr.pl.V.u <- meta.studies %>%
filter(Region == "Verb", EffectType == "Attraction.U", SubjPhi %in% c("Pl"))
meta.attr.pl.V1.u <- meta.studies %>%
filter(Region == "Verb+1", EffectType == "Attraction.U", SubjPhi %in% c("Pl"))
meta.attr.pl.V2.u <- meta.studies %>%
filter(Region == "Verb+2", EffectType == "Attraction.U", SubjPhi %in% c("Pl"))
### Grammatical Sentences
### Singular and Plural Subjects
### Verb, Verb+1 and Verb+2 regions
meta.attr.sg.V.g <- meta.studies %>%
filter(Region == "Verb", EffectType == "Attraction.G", SubjPhi %in% c("Sg"))
meta.attr.sg.V1.g <- meta.studies %>%
filter(Region == "Verb+1", EffectType == "Attraction.G", SubjPhi %in% c("Sg"))
meta.attr.sg.V2.g <- meta.studies %>%
filter(Region == "Verb+2", EffectType == "Attraction.G", SubjPhi %in% c("Sg"))
meta.attr.pl.V.g <- meta.studies %>%
filter(Region == "Verb", EffectType == "Attraction.G", SubjPhi %in% c("Pl"))
meta.attr.pl.V1.g <- meta.studies %>%
filter(Region == "Verb+1", EffectType == "Attraction.G", SubjPhi %in% c("Pl"))
meta.attr.pl.V2.g <- meta.studies %>%
filter(Region == "Verb+2", EffectType == "Attraction.G", SubjPhi %in% c("Pl"))
## Fitting the Meta Analysis FE Model for Number: Ungramm and Gramm Sentences --
## Ungrammatical Sentences - Singular and Plural Subjects
num.attr.sg.V.u <- rma(yi = MeanEffect, vi = VarEffect, method = "FE",
slab = Exp, data = meta.attr.sg.V.u)
num.attr.sg.V1.u <- rma(yi = MeanEffect, vi = VarEffect, method = "FE",
slab = Exp, data = meta.attr.sg.V1.u)
num.attr.sg.V2.u <- rma(yi = MeanEffect, vi = VarEffect, method = "FE",
slab = Exp, data = meta.attr.sg.V2.u)
num.attr.pl.V.u <- rma(yi = MeanEffect, vi = VarEffect, method = "FE",
slab = Exp, data = meta.attr.pl.V.u)
num.attr.pl.V1.u <- rma(yi = MeanEffect, vi = VarEffect, method = "FE",
slab = Exp, data = meta.attr.pl.V1.u)
num.attr.pl.V2.u <- rma(yi = MeanEffect, vi = VarEffect, method = "FE",
slab = Exp, data = meta.attr.pl.V2.u)
## Grammatical Sentences - Singular and Plural Subjects
num.attr.sg.V.g <- rma(yi = MeanEffect, vi = VarEffect, method = "FE",
slab = Exp, data = meta.attr.sg.V.g)
num.attr.sg.V1.g <- rma(yi = MeanEffect, vi = VarEffect, method = "FE",
slab = Exp, data = meta.attr.sg.V1.g)
num.attr.sg.V2.g <- rma(yi = MeanEffect, vi = VarEffect, method = "FE",
slab = Exp, data = meta.attr.sg.V2.g)
num.attr.pl.V.g <- rma(yi = MeanEffect, vi = VarEffect, method = "FE",
slab = Exp, data = meta.attr.pl.V.g)
num.attr.pl.V1.g <- rma(yi = MeanEffect, vi = VarEffect, method = "FE",
slab = Exp, data = meta.attr.pl.V1.g)
num.attr.pl.V2.g <- rma(yi = MeanEffect, vi = VarEffect, method = "FE",
slab = Exp, data = meta.attr.pl.V2.g)
## Forest Plots: Number Attraction Ungrammatical + Grammatical Sentences, Sg ---
### Number Attraction plotting parameters: Singular Subjects Sentences ---------
spr.rt.width  <-  7 #10.4
spr.rt.height <-  7.5 #5.87
plotcex <-  1
plotcex.axis <- 1
plotcex.main <- 1.3
plotcex.axis.lab <- 1
plotdigits <- 0L
num.attr.sg.order <- na.omit(match(chrono.order, num.attr.sg.V.u$slab))
num.attr.pl.order <- na.omit(match(chrono.order, num.attr.pl.V.u$slab))
num.exp.labels.sg <- exp.plotlabels[exp.labels %in% num.attr.sg.V.u$slab]
num.exp.labels.pl <- exp.plotlabels[exp.labels %in% num.attr.pl.V.u$slab]
here(figs.dir, "Attraction-Num-Sg.pdf")
figs.dir
?fs
path(figs.dir, "Attraction-Num-Sg.pdf")
?setEPS
source('~/Dropbox/JML-Revision/AnalysisScripts/07-MetaAnalysis.R')
source('~/Dropbox/JML-Revision/AnalysisScripts/01-Experiment1-Analysis-Frontiers.R')
source('~/Dropbox/JML-Revision/AnalysisScripts/01-Experiment1-Analysis-Frontiers.R')
source('~/Dropbox/JML-Revision/AnalysisScripts/02-Experiment2-Analysis-Frontiers.R')
?mutate_if
source('~/Dropbox/JML-Revision/AnalysisScripts/02-Experiment2-Analysis-Frontiers.R')
source('~/Dropbox/JML-Revision/AnalysisScripts/02-Experiment2-Analysis-Frontiers.R')
source('~/Dropbox/JML-Revision/AnalysisScripts/02-Experiment2-Analysis-Frontiers.R')
source('~/Dropbox/JML-Revision/AnalysisScripts/03-Experiment3-Analysis-Frontiers.R')
source('~/Dropbox/JML-Revision/AnalysisScripts/03-Experiment3-Analysis-Frontiers.R')
source('~/Dropbox/JML-Revision/AnalysisScripts/04-Experiment4-Analysis-Frontiers.R')
source('~/Dropbox/JML-Revision/AnalysisScripts/04-Experiment4-Analysis-Frontiers.R')
source('~/Dropbox/JML-Revision/AnalysisScripts/05-Experiment5-Analysis-Frontiers.R')
source('~/Dropbox/JML-Revision/AnalysisScripts/05-Experiment5-Analysis-Frontiers.R')
source('~/Dropbox/JML-Revision/AnalysisScripts/06-PrepareTucker2015.R')
source('~/Dropbox/JML-Revision/AnalysisScripts/06-PrepareTucker2015.R')
source('~/Dropbox/JML-Revision/AnalysisScripts/00-ReproduceAnalyses-Tucker_et_al_2020.R')
write_dockerfile(maintainer = "diogo-almeida", branch = "main",
r_date = "2020-04-23")
write_dockerfile(maintainer = "diogo-almeida", branch = "main",
r_date = "2020-04-23")
source('~/Dropbox/JML-Revision/AnalysisScripts/00-ReproduceAnalyses-Tucker_et_al_2020.R')
220 - 79
132 / 141
220-80
